{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "609e8570-0a69-4727-935f-081c1b795f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import speech_recognition as sr\n",
    "import language_tool_python\n",
    "import nltk\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "import pyaudio\n",
    "import wave\n",
    "import threading\n",
    "import time\n",
    "import subprocess\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "231c5a02-8e8a-4eac-b556-2b7463c61386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adrij\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f59d1c4-c339-4827-a59a-248a94fa8c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the NLTK data path explicitly\n",
    "nltk.data.path.append('C:\\\\Users\\\\adrij\\\\AppData\\\\Roaming\\\\nltk_data')\n",
    "\n",
    "# Now you can proceed with other imports and function calls\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "61aadf1e-da4a-4ff8-ae2c-aa03a64c97a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\adrij\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c38752af-f168-498d-88e9-036d36916ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for Audio\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "CHUNK = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41acbdca-7c08-46ef-af58-d6060e81b09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to record video\n",
    "def record_video(video_file, duration):\n",
    "    cap = cv2.VideoCapture(0)  # 0 for the default camera\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "    out = cv2.VideoWriter(video_file, fourcc, 20.0, (640, 480))\n",
    "    print(\"Recording video...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < duration:\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            out.write(frame)\n",
    "            cv2.imshow(\"Recording Video\", frame)\n",
    "\n",
    "            # Stop video recording on 'q' key\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    print(\"Video recording stopped.\")\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ad1b18d-5fb6-4799-8a2f-a31a45b66b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to record audio\n",
    "def record_audio(audio_file, duration):\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                        rate=RATE, input=True,\n",
    "                        frames_per_buffer=CHUNK)\n",
    "    print(\"Recording audio...\")\n",
    "    frames = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < duration:\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"Audio recording stopped.\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "    # Save audio to a file\n",
    "    with wave.open(audio_file, \"wb\") as wf:\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "        wf.setframerate(RATE)\n",
    "        wf.writeframes(b\"\".join(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6d8e1e47-5139-43eb-979a-7d42770fb007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract audio from video and perform speech-to-text\n",
    "def video_to_text(video_file, language='en-US'):\n",
    "    audio_file = \"output.wav\"\n",
    "\n",
    "    # Use absolute paths\n",
    "    video_file = os.path.abspath(video_file)\n",
    "    audio_file = os.path.abspath(audio_file)\n",
    "\n",
    "    # Debug: Print paths\n",
    "    print(f\"Video file: {video_file}\")\n",
    "    print(f\"Audio file: {audio_file}\")\n",
    "\n",
    "    # FFmpeg command to extract audio\n",
    "    command = f\"ffmpeg -i \\\"{video_file}\\\" -ar 16000 -ac 1 -y \\\"{audio_file}\\\"\"\n",
    "    os.system(command)\n",
    "\n",
    "    # Verify if the audio file was created\n",
    "    if not os.path.exists(audio_file):\n",
    "        raise FileNotFoundError(f\"Audio file {audio_file} was not created. Check the FFmpeg command.\")\n",
    "\n",
    "    # Initialize recognizer and process audio\n",
    "    recognizer = sr.Recognizer()\n",
    "    try:\n",
    "        with sr.AudioFile(audio_file) as source:\n",
    "            audio = recognizer.record(source)\n",
    "\n",
    "        # Convert audio to text\n",
    "        text = recognizer.recognize_google(audio, language=language)\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Speech recognition could not understand the audio.\"\n",
    "    except sr.RequestError:\n",
    "        return \"Speech recognition service is unavailable.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e53d9e9-caa2-4a24-b484-79cfceddc21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar issues found: 1\n",
      "Flagged Issues:\n",
      " - EN_A_VS_AN: Use “an” instead of ‘a’ if the following word starts with a vowel sound, e.g. ‘an article’, ‘an hour’. (Context: this is a example where grammar mistake are detec...)\n",
      "Grammar Score: 90\n",
      "Issue: Use “an” instead of ‘a’ if the following word starts with a vowel sound, e.g. ‘an article’, ‘an hour’. - Context: this is a example where grammar mistake are detec...\n"
     ]
    }
   ],
   "source": [
    "import language_tool_python\n",
    "\n",
    "def analyze_grammar(text):\n",
    "    \"\"\"\n",
    "    Uses LanguageTool API to check for grammatical issues in the text.\n",
    "    Filters out irrelevant issues like capitalization errors.\n",
    "    \"\"\"\n",
    "    tool = language_tool_python.LanguageTool('en-US')\n",
    "    matches = tool.check(text)\n",
    "    \n",
    "    # Define rules to ignore (e.g., capitalization or punctuation errors)\n",
    "    ignored_rules = {\n",
    "        \"UPPERCASE_SENTENCE_START\",  # Ignore sentences not starting with a capital letter\n",
    "        \"PUNCTUATION_PARAGRAPH_END\",  # Ignore missing punctuation at paragraph end\n",
    "    }\n",
    "\n",
    "    # Filter out matches with ignored rules\n",
    "    relevant_matches = [match for match in matches if match.ruleId not in ignored_rules]\n",
    "    \n",
    "    # Simplistic scoring: reduce score by 10 points for each relevant issue\n",
    "    grammar_score = max(0, 100 - len(relevant_matches) * 10)\n",
    "    \n",
    "    flagged_issues = [(match.ruleId, match.message, match.context) for match in relevant_matches]\n",
    "    \n",
    "    print(f\"Grammar issues found: {len(relevant_matches)}\")\n",
    "    print(\"Flagged Issues:\")\n",
    "    for issue in flagged_issues:\n",
    "        print(f\" - {issue[0]}: {issue[1]} (Context: {issue[2]})\")\n",
    "    \n",
    "    return grammar_score, flagged_issues\n",
    "\n",
    "# Example Usage\n",
    "transcribed_text = \"this is a example where grammar mistake are detected\"\n",
    "score, issues = analyze_grammar(transcribed_text)\n",
    "\n",
    "print(f\"Grammar Score: {score}\")\n",
    "for issue in issues:\n",
    "    print(f\"Issue: {issue[1]} - Context: {issue[2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f95007c-5979-463d-b7df-b84bf37a581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for speaking rate analysis\n",
    "def speaking_rate(text, duration):\n",
    "    \"\"\"\n",
    "    Calculates speaking rate (words per minute).\n",
    "    \"\"\"\n",
    "    word_count = len(word_tokenize(text))\n",
    "    speaking_rate = word_count / (duration / 60)  # words per minute\n",
    "    return speaking_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "26205efe-4499-4b63-b9d6-0bb8cc262d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment, silence\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def analyze_pause_filler(audio_file, transcript):\n",
    "    \"\"\"\n",
    "    Counts filler words from transcript and calculates pause time (>2 sec) from audio.\n",
    "    Handles multi-word fillers correctly.\n",
    "    \"\"\"\n",
    "    # Single word fillers\n",
    "    single_fillers = {\"uh\", \"um\", \"umm\", \"like\", \"well\", \"ah\"}\n",
    "    # Multi-word fillers\n",
    "    multi_fillers = {\"you know\", \"or something\"}\n",
    "\n",
    "    # ---- Count filler words ----\n",
    "    words = word_tokenize(transcript.lower())\n",
    "    filler_count = sum(1 for word in words if word in single_fillers)\n",
    "\n",
    "    # Count multi-word fillers from the transcript text\n",
    "    transcript_lower = transcript.lower()\n",
    "    for phrase in multi_fillers:\n",
    "        filler_count += transcript_lower.count(phrase)\n",
    "\n",
    "    # ---- Detect pauses ----\n",
    "    audio = AudioSegment.from_file(audio_file, format=\"wav\")\n",
    "    silent_chunks = silence.detect_silence(\n",
    "        audio,\n",
    "        min_silence_len=2000,  # 2 seconds\n",
    "        silence_thresh=audio.dBFS - 16\n",
    "    )\n",
    "    total_pause_time = sum((end - start) / 1000.0 for start, end in silent_chunks)\n",
    "\n",
    "    return filler_count, total_pause_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dff2f7d2-af0f-42ec-a4dd-7f74289a6f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pause_score(total_pause_time):\n",
    "    \"\"\"\n",
    "    Calculates a penalty score for total pause time based on durations.\n",
    "    \"\"\"\n",
    "    max_score = 100\n",
    "    penalty_per_second = 15\n",
    "    pause_penalty = min(max_score, total_pause_time * penalty_per_second)\n",
    "    return max(0, max_score - pause_penalty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8e15f482-672b-4ee7-831d-4c36a6460122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "18d30e8b-45a8-4722-b05e-b5becc9efc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_fluency(audio_file, transcript, wpm_target=140):\n",
    "    # Get pauses from analyze_pause_filler\n",
    "    _, total_pause_time = analyze_pause_filler(audio_file, transcript)\n",
    "\n",
    "    # Speaking rate\n",
    "    words = word_tokenize(transcript)\n",
    "    duration_sec = AudioSegment.from_file(audio_file).duration_seconds\n",
    "    wpm = (len(words) / duration_sec) * 60\n",
    "\n",
    "    # Pause penalty\n",
    "    pause_penalty = total_pause_time * 8  # 15 points per sec pause\n",
    "    # WPM penalty\n",
    "    wpm_penalty = abs(wpm_target - wpm) * 0.5  # penalize deviation\n",
    "\n",
    "    fluency_score = max(0, 100 - pause_penalty - wpm_penalty)\n",
    "    return round(fluency_score, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "53ca8bb6-e914-40d2-aae0-7003f8b20566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_completeness(transcript):\n",
    "    words = [w.lower() for w in word_tokenize(transcript) if w.isalpha()]\n",
    "    total_words = len(words)\n",
    "    unique_words = len(set(words))\n",
    "\n",
    "    if total_words == 0:\n",
    "        return 0\n",
    "\n",
    "    diversity_ratio = unique_words / total_words  # 0 to 1\n",
    "    repetition_penalty = (1 - diversity_ratio) * 50  # up to -50 points\n",
    "\n",
    "    completeness_score = max(0, 100 - repetition_penalty)\n",
    "    return round(completeness_score, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2fa2c8f0-aee5-4473-95a6-7e2391248e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pronouncing\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pydub import AudioSegment\n",
    "\n",
    "def analyze_pronunciation(audio_file, transcript):\n",
    "    \"\"\"\n",
    "    Improved pronunciation analysis without Azure.\n",
    "    \"\"\"\n",
    "    # Tokenize & filter\n",
    "    words = [w.lower() for w in word_tokenize(transcript) if w.isalpha()]\n",
    "    total_words = len(words)\n",
    "    if total_words == 0:\n",
    "        return 0, 0, 0, 0\n",
    "\n",
    "    # Accuracy: how many words have dictionary pronunciations\n",
    "    words_with_pron = [w for w in words if pronouncing.phones_for_word(w)]\n",
    "    accuracy_score = (len(words_with_pron) / total_words) * 100\n",
    "\n",
    "    # Fluency: pauses + speaking rate stability\n",
    "    fluency_score = measure_fluency(audio_file, transcript)\n",
    "\n",
    "    # Completeness: lexical diversity & repetition penalty\n",
    "    completeness_score = measure_completeness(transcript)\n",
    "\n",
    "    # Overall: weighted average\n",
    "    overall_score = round(\n",
    "        (accuracy_score * 0.4 + fluency_score * 0.4 + completeness_score * 0.2), 2\n",
    "    )\n",
    "\n",
    "    return overall_score, accuracy_score, fluency_score, completeness_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "24efe802-f0ed-490b-9936-89c6af736341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "def analyze_body_language(video_file):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Unable to open video file.\")\n",
    "        return 0, 0, 0\n",
    "\n",
    "    posture_score = 0\n",
    "    gesture_score = 0\n",
    "    eye_contact_score = 0\n",
    "    total_frames = 0\n",
    "\n",
    "    prev_hand_positions = []\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose, \\\n",
    "         mp_face_mesh.FaceMesh(refine_landmarks=True, min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh, \\\n",
    "         mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            total_frames += 1\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Pose detection\n",
    "            pose_results = pose.process(rgb)\n",
    "            face_results = face_mesh.process(rgb)\n",
    "            hands_results = hands.process(rgb)\n",
    "\n",
    "            # Posture: Check if head is upright\n",
    "            if pose_results.pose_landmarks:\n",
    "                landmarks = pose_results.pose_landmarks.landmark\n",
    "                left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
    "                right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
    "                nose = landmarks[mp_pose.PoseLandmark.NOSE.value]\n",
    "                \n",
    "                # Upright check: shoulders should be roughly horizontal\n",
    "                shoulder_diff = abs(left_shoulder.y - right_shoulder.y)\n",
    "                if shoulder_diff < 0.05:  # Adjust threshold\n",
    "                    posture_score += 1\n",
    "\n",
    "            # Eye Contact: Check iris position\n",
    "            if face_results.multi_face_landmarks:\n",
    "                for face_landmarks in face_results.multi_face_landmarks:\n",
    "                    # Iris landmark indices for MediaPipe Face Mesh\n",
    "                    LEFT_IRIS = [474, 475, 476, 477]\n",
    "                    right_iris_x = face_landmarks.landmark[LEFT_IRIS[0]].x\n",
    "                    if 0.4 < right_iris_x < 0.6:  # Eye is looking forward\n",
    "                        eye_contact_score += 1\n",
    "\n",
    "            # Gesture: Track hand movement\n",
    "            if hands_results.multi_hand_landmarks:\n",
    "                for hand_landmarks in hands_results.multi_hand_landmarks:\n",
    "                    cx = hand_landmarks.landmark[0].x\n",
    "                    cy = hand_landmarks.landmark[0].y\n",
    "                    prev_hand_positions.append((cx, cy))\n",
    "                    if len(prev_hand_positions) > 2:\n",
    "                        dist = ((cx - prev_hand_positions[-2][0])**2 + (cy - prev_hand_positions[-2][1])**2)**0.5\n",
    "                        if dist > 0.02:  # Hand moved significantly\n",
    "                            gesture_score += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Normalize scores\n",
    "    posture_score = min((posture_score / total_frames) * 100, 100)\n",
    "    #gesture_score = min((gesture_score / total_frames) * 100, 100)\n",
    "    eye_contact_score = min((eye_contact_score / total_frames) * 100, 100)\n",
    "\n",
    "    return posture_score, gesture_score, eye_contact_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2bbbec76-aca2-443a-bf2b-a2cc0a0e7e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overall_score(grammar_score, speaking_rate, filler_count, total_pause_time, \n",
    "                            pronunciation_score, accuracy_score, fluency_score):\n",
    "    \"\"\"\n",
    "    Combines all metrics to calculate a final score.\n",
    "    \"\"\"\n",
    "    # Weighting of metrics\n",
    "    weights = {\n",
    "        \"grammar\": 0.3,\n",
    "        \"speaking_rate\": 0.1,\n",
    "        \"filler_words\": 0.1,\n",
    "        \"pause_patterns\": 0.1,\n",
    "        \"pronunciation\": 0.2,\n",
    "        \"accuracy\": 0.1,\n",
    "        \"fluency\": 0.1\n",
    "    }\n",
    "\n",
    "    # Normalize scores and calculate weighted sum\n",
    "    speaking_rate_score = min(100, max(0, 100 - abs(150 - speaking_rate)))  # Target: 150 WPM\n",
    "    filler_word_penalty = max(0, 100 - filler_count * 10)\n",
    "    pause_score = calculate_pause_score(total_pause_time)\n",
    "\n",
    "    overall_score = (\n",
    "        weights[\"grammar\"] * grammar_score +\n",
    "        weights[\"speaking_rate\"] * speaking_rate_score +\n",
    "        weights[\"filler_words\"] * filler_word_penalty +\n",
    "        weights[\"pause_patterns\"] * pause_score +\n",
    "        weights[\"pronunciation\"] * pronunciation_score +\n",
    "        weights[\"accuracy\"] * accuracy_score +\n",
    "        weights[\"fluency\"] * fluency_score\n",
    "    )\n",
    "    return overall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6ffd2c46-a23b-4d74-9996-72edcc2f586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_feedback(speaking_rate, fluency_score, pronunciation_score):\n",
    "    \"\"\"\n",
    "    Provides feedback based on speaking rate, fluency, and pronunciation.\n",
    "    \"\"\"\n",
    "    # Speaking Pace Feedback\n",
    "    if speaking_rate < 80:\n",
    "        pace_feedback = \"You’re speaking too slowly. Consider speeding up to maintain engagement.\"\n",
    "    elif 80 <= speaking_rate < 130:\n",
    "        pace_feedback = \"A moderate pace, but slightly faster speech may improve energy.\"\n",
    "    elif 130 <= speaking_rate < 170:\n",
    "        pace_feedback = \"Perfect! Your speaking pace is in the ideal range.\"\n",
    "    elif 170 <= speaking_rate < 200:\n",
    "        pace_feedback = \"A bit fast. Consider slowing down slightly to enhance clarity.\"\n",
    "    else:\n",
    "        pace_feedback = \"Too fast. Try slowing down for better audience comprehension.\"\n",
    "\n",
    "    # Voice Clarity Feedback\n",
    "    if fluency_score < 70:\n",
    "        fluency_feedback = \"Your speech is not fluent. Focus on reducing hesitations and improving rhythm.\"\n",
    "    elif 70 <= fluency_score < 90:\n",
    "        fluency_feedback = \"Good fluency, but strive for smoother transitions between words.\"\n",
    "    else:\n",
    "        fluency_feedback = \"Excellent fluency! Your speech flows naturally.\"\n",
    "\n",
    "    # Pronunciation Feedback\n",
    "    if pronunciation_score < 70:\n",
    "        pronunciation_feedback = \"Your pronunciation needs improvement. Try practicing individual sounds and word stresses.\"\n",
    "    elif 70 <= pronunciation_score < 90:\n",
    "        pronunciation_feedback = \"Your pronunciation is quite good, but there’s room for slight improvement.\"\n",
    "    else:\n",
    "        pronunciation_feedback = \"Excellent pronunciation! Keep it up.\"\n",
    "\n",
    "    return pace_feedback, fluency_feedback, pronunciation_feedback\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "76f106b1-dc43-4d4d-8ed5-b10bbff73a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording audio...\n",
      "Recording video...\n",
      "Audio recording stopped.\n",
      "Video recording stopped.\n",
      "Video file: C:\\Users\\adrij\\OneDrive\\Desktop\\Plugin\\output.avi\n",
      "Audio file: C:\\Users\\adrij\\OneDrive\\Desktop\\Plugin\\output.wav\n",
      "Generated Transcript:\n",
      "hello my name is address undergraduate student in the department of Electronics and electrical communication engineering and role in his BTech course at Kharagpur I am from Kolkata West Bengal you know sample test\n",
      "Grammar issues found: 2\n",
      "Flagged Issues:\n",
      " - MORFOLOGIK_RULE_EN_US: Possible spelling mistake found. (Context: ...mmunication engineering and role in his BTech course at Kharagpur I am from Kolkata W...)\n",
      " - MORFOLOGIK_RULE_EN_US: Possible spelling mistake found. (Context: ...neering and role in his BTech course at Kharagpur I am from Kolkata West Bengal you know ...)\n",
      "\n",
      "--- Assessment Results ---\n",
      "Grammar Score: 80\n",
      "Grammar Issues: [('MORFOLOGIK_RULE_EN_US', 'Possible spelling mistake found.', '...mmunication engineering and role in his BTech course at Kharagpur I am from Kolkata W...'), ('MORFOLOGIK_RULE_EN_US', 'Possible spelling mistake found.', '...neering and role in his BTech course at Kharagpur I am from Kolkata West Bengal you know ...')]\n",
      "Speaking Rate (WPM): 68.0\n",
      "Filler Word Count: 1\n",
      "Total Pause Time: 14.457 seconds\n",
      "Pronunciation Score: 55.88 (Accuracy: 91.17647058823529, Fluency: 0, Completeness: 97.06)\n",
      "Overall Score: 55.09364705882353\n",
      "You’re speaking too slowly. Consider speeding up to maintain engagement.\n",
      "Your speech is not fluent. Focus on reducing hesitations and improving rhythm.\n",
      "Your pronunciation needs improvement. Try practicing individual sounds and word stresses.\n",
      "\n",
      "--- Body Language Scores ---\n",
      "Posture Score: 71.04/100\n",
      "Eye Contact Score: 76.02/100\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    video_file = \"output.avi\"\n",
    "    audio_file = \"output.wav\"\n",
    "    duration = 30  # seconds\n",
    "\n",
    "    # Step 1: Record Video and Audio Simultaneously\n",
    "    video_thread = threading.Thread(target=record_video, args=(video_file, duration))\n",
    "    audio_thread = threading.Thread(target=record_audio, args=(audio_file, duration))\n",
    "\n",
    "    video_thread.start()\n",
    "    audio_thread.start()\n",
    "\n",
    "    video_thread.join()\n",
    "    audio_thread.join()\n",
    "\n",
    "    # Step 2: Speech-to-Text\n",
    "    transcript = video_to_text(video_file)\n",
    "\n",
    "    print(\"Generated Transcript:\")\n",
    "    print(transcript)\n",
    "    \n",
    " \n",
    "    grammar_score, grammar_issues = analyze_grammar(transcript)\n",
    "    # Step 4: Speaking Rate\n",
    "    speaking_rate_value = speaking_rate(transcript, duration)\n",
    "\n",
    "    # # Step 5: Pause and Filler Word Analysis\n",
    "    # filler_word_count, total_pause_time = analyze_pause_filler(audio_file, azure_region, azure_subscription_key)\n",
    "    filler_word_count, total_pause_time = analyze_pause_filler(audio_file, transcript)\n",
    "\n",
    "    # Step 6: Pronunciation Analysis\n",
    "    pronunciation_score, accuracy_score, fluency_score, completeness_score = analyze_pronunciation(\n",
    "        audio_file, transcript)\n",
    "\n",
    "    # Step 7: Calculate Overall Score\n",
    "    overall_score = calculate_overall_score(\n",
    "        grammar_score, speaking_rate_value, filler_word_count, total_pause_time,\n",
    "        pronunciation_score, accuracy_score, fluency_score\n",
    "    )\n",
    "\n",
    "    # Display Results\n",
    "    print(\"\\n--- Assessment Results ---\")\n",
    "    print(f\"Grammar Score: {grammar_score}\")\n",
    "    print(f\"Grammar Issues: {grammar_issues}\")\n",
    "    print(f\"Speaking Rate (WPM): {speaking_rate_value}\")\n",
    "    print(f\"Filler Word Count: {filler_word_count}\")\n",
    "    print(f\"Total Pause Time: {total_pause_time} seconds\")\n",
    "    print(f\"Pronunciation Score: {pronunciation_score} (Accuracy: {accuracy_score}, Fluency: {fluency_score}, Completeness: {completeness_score})\")\n",
    "    print(f\"Overall Score: {overall_score}\")\n",
    "\n",
    "    pace_feedback, fluency_feedback, pronunciation_feedback = provide_feedback(speaking_rate_value, fluency_score, pronunciation_score)\n",
    "\n",
    "    print(pace_feedback)\n",
    "    print(fluency_feedback)\n",
    "    print(pronunciation_feedback)\n",
    "    # Body language analysis\n",
    "    posture_score, gesture_score, eye_contact_score = analyze_body_language(video_file)\n",
    "    print(\"\\n--- Body Language Scores ---\")\n",
    "    print(f\"Posture Score: {posture_score:.2f}/100\")\n",
    "    #print(f\"Gesture Score: {gesture_score:.2f}/100\")\n",
    "    print(f\"Eye Contact Score: {eye_contact_score:.2f}/100\")\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2521bc7b-2bb3-4a80-a70e-ff96d0ea1eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf11bd1a-2afc-4b3d-970c-9f32e9e0ed89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mp_env)",
   "language": "python",
   "name": "mp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
